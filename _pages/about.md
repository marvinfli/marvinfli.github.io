---
permalink: /
title: "Hi, my name is Marvin."
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm an AI researcher straddling theory and empirics. Right now, I am working on the **theory of generative models** as well as ways to augment the **capabilities and safety of language models**. Earlier this year, I graduated *summa cum laude* from Harvard with a B.A. in Computer Science & Mathematics.

My most recent project was on a unified theoretical framework for feature emergence in generative models (**ICML 2024; ICML 2025 oral**). We explained where high-level features such as reasoning accuracy and toxicity emerge during the sampling trajectories of diffusion models and large language models. This work, also presented in my [thesis](/files/thesis.pdf), was awarded both the **Hoopes Prize** for outstanding undergraduate research and the [**Captain Jonathan Fay Prize**](https://www.radcliffe.harvard.edu/news-and-ideas/harvard-radcliffe-institute-awards-2025-fay-prizes-for-outstanding-theses-of-graduating-class), given to the top three theses across all disciplines at Harvard College.

I love meeting new people and chatting. Please reach out at marvin[dot]fangzhou[dot]li[at]gmail.com.


# Selected Publications  
\* denotes equal contribution

**[Blink of an Eye: A Simple Theory for Feature Localization in Generative Models](https://ar5iv.org/abs/2502.00921)**  
**Marvin Li**, Aayush Karan, Sitan Chen.  
*ICML*, 2025 <span style="color:#d55e5e;">(Oral, top 1% of submissions)</span>  
[arXiv](https://ar5iv.org/abs/2502.00921) / [code](https://github.com/marvinfli/critical-windows-lm)  
A unifying theory showing why and when features suddenly “lock in” during generation in both diffusion and autoregressive models.  

**[Critical Windows: Non-Asymptotic Theory for Feature Emergence in Diffusion Models](https://ar5iv.org/abs/2403.01633)**  
**Marvin Li**, Sitan Chen.  
*ICML*, 2024 \
[arXiv](https://ar5iv.org/abs/2403.01633) / [code](https://github.com/marvinli-harvard/critical-windows) \
Introduces tight, distribution-agnostic bounds pinpointing when image features appear along the diffusion trajectory.  


**[MoPe: Model Perturbation-Based Privacy Attacks on Language Models](https://aclanthology.org/2023.emnlp-main.842)**  
**Marvin Li**\*, Jason Wang\*, Jeffrey Wang\*, Seth Neel.  
*EMNLP* Main Conference, 2023 \
[arXiv](https://arxiv.org/abs/2310.14369)\
Shows that second-order gradient information lets an attacker detect training-set membership far more reliably than loss-only baselines.  

